{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c73af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd23bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# проблема: учит двигаться в одну сторону, напр то 0 до 60\n",
    "# решение: на вых направление [0, 1], скорость [45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eae9ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions = [0, 1] = [-1, 1]\n",
    "\n",
    "class Game:\n",
    "    def __init__(self, angle_degree, distance):\n",
    "        self.angle_degree = angle_degree\n",
    "        self.distance = distance\n",
    "        self.velocity = random.randint(1, 200)\n",
    "        self.distance_to_target = distance\n",
    "        self.record_distance = distance / 4\n",
    "        self.n_games = 0\n",
    "        self.epsilon = 0\n",
    "        \n",
    "    def reset(self):\n",
    "#         self.velocity = random.choice(range(200))\n",
    "        self.velocity = random.randint(1, 200)\n",
    "        self.distance_to_target = self.distance\n",
    "        self.record_distance = self.distance / 4\n",
    "#         print('reset')\n",
    "        return np.array([self.angle_degree, self.distance, self.distance_to_target, self.velocity])\n",
    "\n",
    "    def calculate_distance_to_target(self, v0, angle_degree):\n",
    "        g = 9.8\n",
    "        angle_radian = math.radians(angle_degree)\n",
    "        distance = (v0**2 * math.sin(2 * angle_radian)) / g\n",
    "        return distance\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        self.n_games += 1\n",
    "\n",
    "        if action == 0:\n",
    "            self.velocity -= 1\n",
    "        else:\n",
    "            self.velocity += 1\n",
    "\n",
    "        # move\n",
    "        calc_distance = self.calculate_distance_to_target(self.velocity, self.angle_degree)\n",
    "\n",
    "        new_distance_to_target = abs(self.distance - calc_distance)\n",
    "\n",
    "        # reward\n",
    "#         reward = abs((self.distance - distance) / distance_to_target**2)\n",
    "        if new_distance_to_target < self.distance_to_target:\n",
    "            self.distance_to_target = new_distance_to_target\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "            \n",
    "        if new_distance_to_target < self.record_distance or new_distance_to_target > self.distance * 2:\n",
    "            self.record_distance = new_distance_to_target\n",
    "            is_done = True\n",
    "        else:\n",
    "            is_done = False\n",
    "            \n",
    "        # update ui\n",
    "#         print('distance_to_target', new_distance_to_target)\n",
    "        \n",
    "        state = np.array([self.angle_degree, self.distance, new_distance_to_target, self.velocity])\n",
    "\n",
    "        return state, reward, is_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b50bbbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d64510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, state_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d06a6fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of rewards in episode\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "# one step in episode\n",
    "Step = namedtuple('Step', field_names=['state', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2b17a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env, model, batch_size):\n",
    "    batch = [] \n",
    "    episode_reward = 0.0\n",
    "    episode_steps = [] \n",
    "    state = env.reset()\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    while True:\n",
    "        env.epsilon = 180 - env.n_games\n",
    "        if random.randint(0, 1200) < env.epsilon:\n",
    "            action = random.randint(0, 1)\n",
    "        else:\n",
    "            state_v = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "            pred = model(state_v)\n",
    "            act_probs_v = softmax(pred)\n",
    "            act_probs = act_probs_v.data.numpy()[0]\n",
    "\n",
    "            action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        \n",
    "        next_state, reward, is_done = env.step(action)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        episode_steps.append(Step(state=state, action=action))\n",
    "        \n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_state = env.reset()\n",
    "            \n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "                \n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d751799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile) \n",
    "    reward_mean = float(np.mean(rewards))\n",
    "    \n",
    "    train_states = []\n",
    "    train_actions = []\n",
    "    \n",
    "    for el in batch:\n",
    "        if el.reward < reward_bound:\n",
    "            continue\n",
    "        \n",
    "        train_states.extend(map(lambda step: step.state, el.steps))\n",
    "        train_actions.extend(map(lambda step: step.action, el.steps))\n",
    "        \n",
    "    train_states_v = torch.tensor(train_states, dtype=torch.float)\n",
    "    train_actions_v = torch.tensor(train_actions, dtype=torch.long)\n",
    "    \n",
    "    return train_states_v, train_actions_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e96ac244",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1rati\\AppData\\Local\\Temp\\ipykernel_11916\\1253192135.py:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  train_states_v = torch.tensor(train_states, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.625, reward_mean=2.5, reward_bound=0.0\n",
      "1: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "2: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "3: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "4: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "5: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "6: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "7: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "8: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "9: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "10: loss=0.000, reward_mean=2.9, reward_bound=0.0\n",
      "11: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "12: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "13: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "14: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "15: loss=0.000, reward_mean=3.1, reward_bound=0.0\n",
      "16: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "17: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "18: loss=0.000, reward_mean=2.4, reward_bound=1.0\n",
      "19: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "20: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "21: loss=0.000, reward_mean=1.5, reward_bound=0.0\n",
      "22: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "23: loss=0.000, reward_mean=3.5, reward_bound=0.9\n",
      "24: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "25: loss=0.000, reward_mean=3.0, reward_bound=0.9\n",
      "26: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "27: loss=0.000, reward_mean=1.5, reward_bound=0.0\n",
      "28: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "29: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "30: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "31: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "32: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "33: loss=0.000, reward_mean=1.5, reward_bound=0.0\n",
      "34: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "35: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "36: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "37: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "38: loss=0.000, reward_mean=3.0, reward_bound=0.0\n",
      "39: loss=0.000, reward_mean=2.9, reward_bound=0.0\n",
      "40: loss=0.000, reward_mean=3.2, reward_bound=0.0\n",
      "41: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "42: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "43: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "44: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "45: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "46: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "47: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "48: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "49: loss=0.000, reward_mean=1.4, reward_bound=0.0\n",
      "50: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "51: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "52: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "53: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "54: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "55: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "56: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "57: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "58: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "59: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "60: loss=0.000, reward_mean=0.5, reward_bound=0.0\n",
      "61: loss=0.000, reward_mean=2.3, reward_bound=0.9\n",
      "62: loss=0.000, reward_mean=3.4, reward_bound=0.0\n",
      "63: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "64: loss=0.000, reward_mean=1.2, reward_bound=0.0\n",
      "65: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "66: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "67: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "68: loss=0.000, reward_mean=1.7, reward_bound=0.0\n",
      "69: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "70: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "71: loss=0.000, reward_mean=2.8, reward_bound=0.0\n",
      "72: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "73: loss=0.000, reward_mean=1.3, reward_bound=0.0\n",
      "74: loss=0.000, reward_mean=1.7, reward_bound=0.0\n",
      "75: loss=0.000, reward_mean=1.7, reward_bound=0.0\n",
      "76: loss=0.000, reward_mean=1.5, reward_bound=0.0\n",
      "77: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "78: loss=0.000, reward_mean=2.9, reward_bound=0.0\n",
      "79: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "80: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "81: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "82: loss=0.000, reward_mean=3.1, reward_bound=0.0\n",
      "83: loss=0.000, reward_mean=3.1, reward_bound=0.0\n",
      "84: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "85: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "86: loss=0.000, reward_mean=2.9, reward_bound=0.9\n",
      "87: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "88: loss=0.000, reward_mean=1.5, reward_bound=0.0\n",
      "89: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "90: loss=0.000, reward_mean=2.8, reward_bound=0.0\n",
      "91: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "92: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "93: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "94: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "95: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "96: loss=0.000, reward_mean=3.3, reward_bound=0.0\n",
      "97: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "98: loss=0.000, reward_mean=3.0, reward_bound=0.0\n",
      "99: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "100: loss=0.000, reward_mean=3.3, reward_bound=0.0\n",
      "101: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "102: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "103: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "104: loss=0.000, reward_mean=3.1, reward_bound=0.0\n",
      "105: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "106: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "107: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "108: loss=0.000, reward_mean=1.7, reward_bound=0.0\n",
      "109: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "110: loss=0.000, reward_mean=3.2, reward_bound=0.0\n",
      "111: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "112: loss=0.000, reward_mean=3.0, reward_bound=0.0\n",
      "113: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "114: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "115: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "116: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "117: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "118: loss=0.000, reward_mean=3.5, reward_bound=0.0\n",
      "119: loss=0.000, reward_mean=3.4, reward_bound=0.0\n",
      "120: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "121: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "122: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "123: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "124: loss=0.000, reward_mean=1.2, reward_bound=0.0\n",
      "125: loss=0.000, reward_mean=3.0, reward_bound=0.0\n",
      "126: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "127: loss=0.000, reward_mean=1.3, reward_bound=0.0\n",
      "128: loss=0.000, reward_mean=2.9, reward_bound=0.0\n",
      "129: loss=0.000, reward_mean=1.4, reward_bound=0.0\n",
      "130: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "131: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "132: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "133: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "134: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "135: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "136: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "137: loss=0.000, reward_mean=1.7, reward_bound=0.0\n",
      "138: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "139: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "140: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "141: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "142: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "143: loss=0.000, reward_mean=2.8, reward_bound=0.0\n",
      "144: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "145: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "146: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "147: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "148: loss=0.000, reward_mean=1.5, reward_bound=0.0\n",
      "149: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "150: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "151: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "152: loss=0.000, reward_mean=1.5, reward_bound=0.0\n",
      "153: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "154: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "155: loss=0.000, reward_mean=3.5, reward_bound=0.0\n",
      "156: loss=0.000, reward_mean=1.7, reward_bound=0.0\n",
      "157: loss=0.000, reward_mean=1.4, reward_bound=0.0\n",
      "158: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "159: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "160: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "161: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "162: loss=0.000, reward_mean=2.8, reward_bound=0.0\n",
      "163: loss=0.000, reward_mean=2.4, reward_bound=0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164: loss=0.000, reward_mean=1.7, reward_bound=0.0\n",
      "165: loss=0.000, reward_mean=2.9, reward_bound=0.0\n",
      "166: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "167: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "168: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "169: loss=0.000, reward_mean=2.7, reward_bound=1.0\n",
      "170: loss=0.000, reward_mean=2.9, reward_bound=0.0\n",
      "171: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "172: loss=0.000, reward_mean=1.5, reward_bound=0.0\n",
      "173: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "174: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "175: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "176: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "177: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "178: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "179: loss=0.000, reward_mean=3.2, reward_bound=1.0\n",
      "180: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "181: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "182: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "183: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "184: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "185: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "186: loss=0.000, reward_mean=1.3, reward_bound=0.0\n",
      "187: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "188: loss=0.000, reward_mean=1.7, reward_bound=0.0\n",
      "189: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "190: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "191: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "192: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "193: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "194: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "195: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "196: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "197: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "198: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "199: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "200: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "201: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "202: loss=0.000, reward_mean=3.2, reward_bound=0.0\n",
      "203: loss=0.000, reward_mean=3.2, reward_bound=0.9\n",
      "204: loss=0.000, reward_mean=3.1, reward_bound=0.0\n",
      "205: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "206: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "207: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "208: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "209: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "210: loss=0.000, reward_mean=2.8, reward_bound=0.0\n",
      "211: loss=0.000, reward_mean=1.3, reward_bound=0.0\n",
      "212: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "213: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "214: loss=0.000, reward_mean=3.0, reward_bound=0.0\n",
      "215: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "216: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "217: loss=0.000, reward_mean=1.7, reward_bound=0.0\n",
      "218: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "219: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "220: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "221: loss=0.000, reward_mean=1.5, reward_bound=0.0\n",
      "222: loss=0.000, reward_mean=1.3, reward_bound=0.0\n",
      "223: loss=0.000, reward_mean=2.7, reward_bound=1.0\n",
      "224: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "225: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "226: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "227: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "228: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "229: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "230: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "231: loss=0.000, reward_mean=2.3, reward_bound=1.0\n",
      "232: loss=0.000, reward_mean=2.8, reward_bound=0.0\n",
      "233: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "234: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "235: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "236: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "237: loss=0.000, reward_mean=3.1, reward_bound=0.0\n",
      "238: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "239: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "240: loss=0.000, reward_mean=1.5, reward_bound=0.0\n",
      "241: loss=0.000, reward_mean=3.4, reward_bound=0.0\n",
      "242: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "243: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "244: loss=0.000, reward_mean=2.8, reward_bound=0.0\n",
      "245: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "246: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "247: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "248: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "249: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "250: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "251: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "252: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "253: loss=0.000, reward_mean=3.1, reward_bound=0.0\n",
      "254: loss=0.000, reward_mean=1.7, reward_bound=0.0\n",
      "255: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "256: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "257: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "258: loss=0.000, reward_mean=3.0, reward_bound=0.9\n",
      "259: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "260: loss=0.000, reward_mean=1.5, reward_bound=0.0\n",
      "261: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "262: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "263: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "264: loss=0.000, reward_mean=1.5, reward_bound=0.0\n",
      "265: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "266: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "267: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "268: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "269: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "270: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "271: loss=0.000, reward_mean=2.9, reward_bound=0.0\n",
      "272: loss=0.000, reward_mean=2.9, reward_bound=0.0\n",
      "273: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "274: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "275: loss=0.000, reward_mean=1.5, reward_bound=0.0\n",
      "276: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "277: loss=0.000, reward_mean=3.8, reward_bound=0.0\n",
      "278: loss=0.000, reward_mean=3.5, reward_bound=0.0\n",
      "279: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "280: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "281: loss=0.000, reward_mean=1.2, reward_bound=0.0\n",
      "282: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "283: loss=0.000, reward_mean=3.0, reward_bound=0.0\n",
      "284: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "285: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "286: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "287: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "288: loss=0.000, reward_mean=1.3, reward_bound=0.0\n",
      "289: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "290: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "291: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "292: loss=0.000, reward_mean=3.3, reward_bound=0.0\n",
      "293: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "294: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "295: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "296: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "297: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "298: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "299: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "300: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "301: loss=0.000, reward_mean=1.7, reward_bound=0.0\n",
      "302: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "303: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "304: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "305: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "306: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "307: loss=0.000, reward_mean=1.4, reward_bound=0.0\n",
      "308: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "309: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "310: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "311: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "312: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "313: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "314: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "315: loss=0.000, reward_mean=1.4, reward_bound=0.0\n",
      "316: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "317: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "318: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "319: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "320: loss=0.000, reward_mean=3.5, reward_bound=0.0\n",
      "321: loss=0.000, reward_mean=2.9, reward_bound=0.0\n",
      "322: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "323: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "324: loss=0.000, reward_mean=1.8, reward_bound=0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325: loss=0.000, reward_mean=2.9, reward_bound=0.0\n",
      "326: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "327: loss=0.000, reward_mean=1.7, reward_bound=0.0\n",
      "328: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "329: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "330: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "331: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "332: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "333: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "334: loss=0.000, reward_mean=3.0, reward_bound=0.0\n",
      "335: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "336: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "337: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "338: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "339: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "340: loss=0.000, reward_mean=1.7, reward_bound=0.0\n",
      "341: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "342: loss=0.000, reward_mean=3.2, reward_bound=0.0\n",
      "343: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "344: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "345: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "346: loss=0.000, reward_mean=3.0, reward_bound=0.0\n",
      "347: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "348: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "349: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "350: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "351: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "352: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "353: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "354: loss=0.000, reward_mean=2.9, reward_bound=0.0\n",
      "355: loss=0.000, reward_mean=3.1, reward_bound=0.0\n",
      "356: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "357: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "358: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "359: loss=0.000, reward_mean=3.1, reward_bound=0.0\n",
      "360: loss=0.000, reward_mean=1.5, reward_bound=0.0\n",
      "361: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "362: loss=0.000, reward_mean=1.5, reward_bound=0.0\n",
      "363: loss=0.000, reward_mean=2.9, reward_bound=0.0\n",
      "364: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "365: loss=0.000, reward_mean=3.0, reward_bound=0.0\n",
      "366: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "367: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "368: loss=0.000, reward_mean=3.4, reward_bound=1.0\n",
      "369: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "370: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "371: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "372: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "373: loss=0.000, reward_mean=2.8, reward_bound=0.0\n",
      "374: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "375: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "376: loss=0.000, reward_mean=2.8, reward_bound=0.0\n",
      "377: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "378: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "379: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "380: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "381: loss=0.000, reward_mean=2.9, reward_bound=0.0\n",
      "382: loss=0.000, reward_mean=3.2, reward_bound=0.0\n",
      "383: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "384: loss=0.000, reward_mean=3.0, reward_bound=0.0\n",
      "385: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "386: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "387: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "388: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "389: loss=0.000, reward_mean=3.1, reward_bound=0.0\n",
      "390: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "391: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "392: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "393: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "394: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "395: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "396: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "397: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "398: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "399: loss=0.000, reward_mean=3.0, reward_bound=0.0\n",
      "400: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "401: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "402: loss=0.000, reward_mean=3.2, reward_bound=0.0\n",
      "403: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "404: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "405: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "406: loss=0.000, reward_mean=1.5, reward_bound=0.0\n",
      "407: loss=0.000, reward_mean=1.4, reward_bound=0.0\n",
      "408: loss=0.000, reward_mean=1.5, reward_bound=0.0\n",
      "409: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "410: loss=0.000, reward_mean=3.5, reward_bound=0.0\n",
      "411: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "412: loss=0.000, reward_mean=3.0, reward_bound=0.0\n",
      "413: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "414: loss=0.000, reward_mean=3.3, reward_bound=1.0\n",
      "415: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "416: loss=0.000, reward_mean=1.7, reward_bound=0.0\n",
      "417: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "418: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "419: loss=0.000, reward_mean=3.3, reward_bound=0.0\n",
      "420: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "421: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "422: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "423: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "424: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "425: loss=0.000, reward_mean=2.9, reward_bound=0.0\n",
      "426: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "427: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "428: loss=0.000, reward_mean=1.4, reward_bound=0.0\n",
      "429: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "430: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "431: loss=0.000, reward_mean=1.7, reward_bound=0.0\n",
      "432: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "433: loss=0.000, reward_mean=1.5, reward_bound=0.0\n",
      "434: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "435: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "436: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "437: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "438: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "439: loss=0.000, reward_mean=2.8, reward_bound=0.0\n",
      "440: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "441: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "442: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "443: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "444: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "445: loss=0.000, reward_mean=3.2, reward_bound=0.0\n",
      "446: loss=0.000, reward_mean=3.1, reward_bound=0.0\n",
      "447: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "448: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "449: loss=0.000, reward_mean=3.8, reward_bound=0.9\n",
      "450: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "451: loss=0.000, reward_mean=1.7, reward_bound=0.0\n",
      "452: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "453: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "454: loss=0.000, reward_mean=3.0, reward_bound=0.0\n",
      "455: loss=0.000, reward_mean=1.4, reward_bound=0.0\n",
      "456: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "457: loss=0.000, reward_mean=3.8, reward_bound=1.0\n",
      "458: loss=0.000, reward_mean=1.3, reward_bound=0.0\n",
      "459: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "460: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "461: loss=0.000, reward_mean=3.6, reward_bound=0.0\n",
      "462: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "463: loss=0.000, reward_mean=3.6, reward_bound=1.0\n",
      "464: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "465: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "466: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "467: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "468: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "469: loss=0.000, reward_mean=2.3, reward_bound=0.0\n",
      "470: loss=0.000, reward_mean=1.7, reward_bound=0.0\n",
      "471: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "472: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "473: loss=0.000, reward_mean=3.5, reward_bound=1.0\n",
      "474: loss=0.000, reward_mean=2.8, reward_bound=1.0\n",
      "475: loss=0.000, reward_mean=3.5, reward_bound=1.0\n",
      "476: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "477: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "478: loss=0.000, reward_mean=2.8, reward_bound=0.0\n",
      "479: loss=0.000, reward_mean=2.9, reward_bound=0.0\n",
      "480: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "481: loss=0.000, reward_mean=2.7, reward_bound=0.9\n",
      "482: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "483: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "484: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "485: loss=0.000, reward_mean=1.9, reward_bound=0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486: loss=0.000, reward_mean=2.2, reward_bound=0.0\n",
      "487: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "488: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "489: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "490: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "491: loss=0.000, reward_mean=2.0, reward_bound=0.0\n",
      "492: loss=0.000, reward_mean=2.4, reward_bound=0.0\n",
      "493: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "494: loss=0.000, reward_mean=1.7, reward_bound=0.0\n",
      "495: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "496: loss=0.000, reward_mean=1.7, reward_bound=0.0\n",
      "497: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "498: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "499: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "500: loss=0.000, reward_mean=1.8, reward_bound=0.0\n",
      "501: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "502: loss=0.000, reward_mean=1.6, reward_bound=0.0\n",
      "503: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "504: loss=0.000, reward_mean=2.7, reward_bound=0.0\n",
      "505: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "506: loss=0.000, reward_mean=1.5, reward_bound=0.0\n",
      "507: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "508: loss=0.000, reward_mean=2.6, reward_bound=0.0\n",
      "509: loss=0.000, reward_mean=2.8, reward_bound=0.0\n",
      "510: loss=0.000, reward_mean=1.9, reward_bound=0.0\n",
      "511: loss=0.000, reward_mean=1.4, reward_bound=0.0\n",
      "512: loss=0.000, reward_mean=1.7, reward_bound=0.0\n",
      "513: loss=0.000, reward_mean=2.1, reward_bound=0.0\n",
      "514: loss=0.000, reward_mean=2.5, reward_bound=0.0\n",
      "515: loss=0.000, reward_mean=1.9, reward_bound=0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# loss_fn = nn.MSELoss()\u001b[39;00m\n\u001b[0;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(params\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(iterate_batches(env, model, BATCH_SIZE)):\n\u001b[0;32m     15\u001b[0m     states_v, actions_v, reward_bound, reward_mean \u001b[38;5;241m=\u001b[39m filter_batch(batch, PERCENTILE)\n\u001b[0;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m, in \u001b[0;36miterate_batches\u001b[1;34m(env, model, batch_size)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     state_v \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_v\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     act_probs_v \u001b[38;5;241m=\u001b[39m softmax(pred)\n\u001b[0;32m     16\u001b[0m     act_probs \u001b[38;5;241m=\u001b[39m act_probs_v\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mC:\\Python3.9.13\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python3.9.13\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\Python3.9.13\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mC:\\Python3.9.13\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\Python3.9.13\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = Game(30, 100)\n",
    "state_size = 4\n",
    "n_actions = 2\n",
    "\n",
    "log_loss = []\n",
    "log_reward_bound = []\n",
    "log_reward_mean = []\n",
    "\n",
    "model = Net(state_size, HIDDEN_SIZE, n_actions)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.01)\n",
    "\n",
    "for i, batch in enumerate(iterate_batches(env, model, BATCH_SIZE)):\n",
    "    states_v, actions_v, reward_bound, reward_mean = filter_batch(batch, PERCENTILE)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    action_scores_v = model(states_v)\n",
    "    loss_v = loss_fn(action_scores_v, actions_v)\n",
    "    \n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f' % (i, loss_v.item(), reward_mean, reward_bound))\n",
    "    \n",
    "    log_loss.append(loss_v.item())\n",
    "    log_reward_bound.append(reward_bound)\n",
    "    log_reward_mean.append(reward_mean)\n",
    "                           \n",
    "    if reward_mean > 199:\n",
    "        print('Solved!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0acfbb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log_reward_mean, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9568b95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
